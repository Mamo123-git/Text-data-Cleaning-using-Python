{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#ps = PorterStemmer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring Snowball Stemmer\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    " # declaring WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incident</th>\n",
       "      <th>Email_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5632054</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5632083</td>\n",
       "      <td>Response Channel: EmailEmail   01/02/2020 02:2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5632093</td>\n",
       "      <td>Response Channel: EmailEmail   01/21/2020 08:4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5632101</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5632108</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>5670263</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>5670303</td>\n",
       "      <td>Response Channel: EmailEmail   02/03/2020 05:2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>5670353</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>5670463</td>\n",
       "      <td>Response Channel: EmailEmail   02/05/2020 02:5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>5670478</td>\n",
       "      <td>Response Channel: EmailEmail   02/01/2020 01:5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Incident                                         Email_Text\n",
       "0     5632054  Auto Response Channel: No ChannelNo Channel   ...\n",
       "1     5632083  Response Channel: EmailEmail   01/02/2020 02:2...\n",
       "2     5632093  Response Channel: EmailEmail   01/21/2020 08:4...\n",
       "3     5632101  Auto Response Channel: No ChannelNo Channel   ...\n",
       "4     5632108  Auto Response Channel: No ChannelNo Channel   ...\n",
       "..        ...                                                ...\n",
       "822   5670263  Auto Response Channel: No ChannelNo Channel   ...\n",
       "823   5670303  Response Channel: EmailEmail   02/03/2020 05:2...\n",
       "824   5670353  Auto Response Channel: No ChannelNo Channel   ...\n",
       "825   5670463  Response Channel: EmailEmail   02/05/2020 02:5...\n",
       "826   5670478  Response Channel: EmailEmail   02/01/2020 01:5...\n",
       "\n",
       "[827 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data\n",
    "raw_data = pd.read_excel('Email Data.xlsx')\n",
    "data = raw_data[['Incident','Email_Text']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# split email using date wise in the format  01/13/2020 02:20\n",
    "\n",
    "def split_date(text):\n",
    "    text = re.split(r' \\d?\\d/\\d?\\d/\\d{4} [0-1][0-9]:[0-5][0-9]' , text)\n",
    "    return text\n",
    "\n",
    "data['Split_Emails']= ''\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "   # print (i)\n",
    "    data['Split_Emails'][i] = split_date(data['Email_Text'][i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incident</th>\n",
       "      <th>Email_Text</th>\n",
       "      <th>Split_Emails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5632054</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "      <td>[Auto Response Channel: No ChannelNo Channel  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5632083</td>\n",
       "      <td>Response Channel: EmailEmail   01/02/2020 02:2...</td>\n",
       "      <td>[Response Channel: EmailEmail  ,  AMJames Parv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5632093</td>\n",
       "      <td>Response Channel: EmailEmail   01/21/2020 08:4...</td>\n",
       "      <td>[Response Channel: EmailEmail  ,  PMTom Singh,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5632101</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "      <td>[Auto Response Channel: No ChannelNo Channel  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5632108</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "      <td>[Auto Response Channel: No ChannelNo Channel  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>5670263</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "      <td>[Auto Response Channel: No ChannelNo Channel  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>5670303</td>\n",
       "      <td>Response Channel: EmailEmail   02/03/2020 05:2...</td>\n",
       "      <td>[Response Channel: EmailEmail  ,  AMJames Parv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>5670353</td>\n",
       "      <td>Auto Response Channel: No ChannelNo Channel   ...</td>\n",
       "      <td>[Auto Response Channel: No ChannelNo Channel  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>5670463</td>\n",
       "      <td>Response Channel: EmailEmail   02/05/2020 02:5...</td>\n",
       "      <td>[Response Channel: EmailEmail  ,  AMDerick Upa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>5670478</td>\n",
       "      <td>Response Channel: EmailEmail   02/01/2020 01:5...</td>\n",
       "      <td>[Response Channel: EmailEmail  ,  PMEllie Barm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Incident                                         Email_Text  \\\n",
       "0     5632054  Auto Response Channel: No ChannelNo Channel   ...   \n",
       "1     5632083  Response Channel: EmailEmail   01/02/2020 02:2...   \n",
       "2     5632093  Response Channel: EmailEmail   01/21/2020 08:4...   \n",
       "3     5632101  Auto Response Channel: No ChannelNo Channel   ...   \n",
       "4     5632108  Auto Response Channel: No ChannelNo Channel   ...   \n",
       "..        ...                                                ...   \n",
       "822   5670263  Auto Response Channel: No ChannelNo Channel   ...   \n",
       "823   5670303  Response Channel: EmailEmail   02/03/2020 05:2...   \n",
       "824   5670353  Auto Response Channel: No ChannelNo Channel   ...   \n",
       "825   5670463  Response Channel: EmailEmail   02/05/2020 02:5...   \n",
       "826   5670478  Response Channel: EmailEmail   02/01/2020 01:5...   \n",
       "\n",
       "                                          Split_Emails  \n",
       "0    [Auto Response Channel: No ChannelNo Channel  ...  \n",
       "1    [Response Channel: EmailEmail  ,  AMJames Parv...  \n",
       "2    [Response Channel: EmailEmail  ,  PMTom Singh,...  \n",
       "3    [Auto Response Channel: No ChannelNo Channel  ...  \n",
       "4    [Auto Response Channel: No ChannelNo Channel  ...  \n",
       "..                                                 ...  \n",
       "822  [Auto Response Channel: No ChannelNo Channel  ...  \n",
       "823  [Response Channel: EmailEmail  ,  AMJames Parv...  \n",
       "824  [Auto Response Channel: No ChannelNo Channel  ...  \n",
       "825  [Response Channel: EmailEmail  ,  AMDerick Upa...  \n",
       "826  [Response Channel: EmailEmail  ,  PMEllie Barm...  \n",
       "\n",
       "[827 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# padding a list to form the same length and same columns\n",
    "# It is difficult to split into different columns as the column length is not equal\n",
    "    \n",
    "data['NewSplit_Emails'] = ''\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    data['NewSplit_Emails'][i] = data['Split_Emails'][i] + [''] * (42 - len(data['Split_Emails'][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# checking the length of the column\n",
    "\n",
    "data['length'] = data['Split_Emails'].apply(lambda x: len(x))\n",
    "\n",
    "# to extend the split data into different columns\n",
    "   \n",
    "data2 = data.iloc[:,[3]]\n",
    "\n",
    "df2 = pd.DataFrame(data2.NewSplit_Emails.values.tolist(), index= data2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_excel('sp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the last email from each text\n",
    "\n",
    "df2['lastemail']=''\n",
    "for row in range(df2.shape[0]):\n",
    "    col= data['length'][row]\n",
    "    col = col-1\n",
    "    df2['lastemail'][row]=df2.iloc[row,col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final dataframe by concatenating 2 dataframes\n",
    "\n",
    "main_data = pd.concat([data, df2], axis=1, sort=False)\n",
    "# result with all the columns\n",
    "main_data = main_data.drop(['length', 'NewSplit_Emails','Split_Emails'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the required result with last column\n",
    "#result dataframe with only the last column\n",
    "result = main_data[['Incident','Email_Text','lastemail']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# removing Best regards and everything before Dear word in each cell\n",
    "\n",
    "def data_cleaning(text):\n",
    "    try:\n",
    "        text = re.sub('\\n+',' ',text)\n",
    "        text = re.sub(r' Best regards.*$','',text)\n",
    "        text = re.sub(r' Sincerely.*$','',text)\n",
    "        text = re.sub(r'.*Dear ','',text)\n",
    "        text = re.sub(r'.*.pdf','',text)\n",
    "        text = re.sub(r'.*.JPG','',text) \n",
    "        text = re.sub(r'.*.jpg','',text)\n",
    "        text = re.sub(r'.*.jpeg','',text)\n",
    "        text = re.sub(r'.*.png','',text)\n",
    "        text = re.sub(r'.*.bmp','',text)\n",
    "        text = re.sub(r'.*.gif','',text)\n",
    "        text = re.sub(r'.*.docx','',text)\n",
    "        text = re.sub(r'.*.PNG','',text)\n",
    "        text = text.strip()\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "for col in result:\n",
    "    #if col=='Email_Text' or col== 'Incident':\n",
    "    #    continue\n",
    "    for i in range(result.shape[0]):\n",
    "        #print(result[col][i])\n",
    "        text = result[col][i]\n",
    "        result[col][i] = data_cleaning(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.to_excel('outputfile.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NTLK - NER  to remove names from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef preprocess_text(text):\\n    \\n    This function takes a text. Split it in tokens using word_tokenize.  \\n    And then tags them using pos_tag from NLTK module. \\n    It outputs a list of tuples. Each tuple consists of a word and the tag with its  \\n    part of speech. \\n    \\n    # Get the tokens \\n    tokens = nltk.word_tokenize(text) \\n    # Tags the tokens \\n    tagging = nltk.pos_tag(tokens) \\n    # Returns the list of tuples \\n    return tagging \\n    \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    This function takes a text. Split it in tokens using word_tokenize.  \n",
    "    And then tags them using pos_tag from NLTK module. \n",
    "    It outputs a list of tuples. Each tuple consists of a word and the tag with its  \n",
    "    part of speech. \n",
    "    \n",
    "    # Get the tokens \n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    # Tags the tokens \n",
    "    tagging = nltk.pos_tag(tokens) \n",
    "    # Returns the list of tuples \n",
    "    return tagging \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef Named_entity_recognition(text):                \\n    # Split and label the text \\n    label_text = preprocess_text(text) \\n    #print\\n    entity1 = []\\n    #Print first 20 tuples, 5 per line \\n    for i in range(0, 20, 5): \\n        print(label_text[i], label_text[i+1], label_text[i+2], label_text[i+3],  \\n        label_text[i+4])\\n        \\n    # Define the rule\\n    rule = \"NP: {<DT>?<JJ>*<NNP>+}\" \\n    \\n    # We define the parser using the rule \\n    parser = nltk.RegexpParser(rule) \\n\\n    # Apply to the tagged words \\n    result = parser.parse(label_text)\\n    #print (result)\\n      \\n    # Print only the chunks\\n    entities = nltk.ne_chunk(label_text,binary = False)\\n    #for entity in entities:\\n    for entity in entities:\\n        print(entity)\\n        if type(entity) == nltk.tree.Tree:\\n            #if entity.label_ == PERSON:\\n            print(entity)\\n            entity1.append(entity)\\n     \\n    return entity1\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def Named_entity_recognition(text):                \n",
    "    # Split and label the text \n",
    "    label_text = preprocess_text(text) \n",
    "    #print\n",
    "    entity1 = []\n",
    "    #Print first 20 tuples, 5 per line \n",
    "    for i in range(0, 20, 5): \n",
    "        print(label_text[i], label_text[i+1], label_text[i+2], label_text[i+3],  \n",
    "        label_text[i+4])\n",
    "        \n",
    "    # Define the rule\n",
    "    rule = \"NP: {<DT>?<JJ>*<NNP>+}\" \n",
    "    \n",
    "    # We define the parser using the rule \n",
    "    parser = nltk.RegexpParser(rule) \n",
    "\n",
    "    # Apply to the tagged words \n",
    "    result = parser.parse(label_text)\n",
    "    #print (result)\n",
    "      \n",
    "    # Print only the chunks\n",
    "    entities = nltk.ne_chunk(label_text,binary = False)\n",
    "    #for entity in entities:\n",
    "    for entity in entities:\n",
    "        print(entity)\n",
    "        if type(entity) == nltk.tree.Tree:\n",
    "            #if entity.label_ == PERSON:\n",
    "            print(entity)\n",
    "            entity1.append(entity)\n",
    "     \n",
    "    return entity1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# concatenating all the text required into a long string to find the names of Persons in the string\\nstrings = ''\\nfor col in result:\\n    if col=='Email_Text' or col== 'Incident' or col=='0':\\n        continue\\n    for i in range(result.shape[0]):\\n        # print(result[col][i])\\n        text = result[col][i]\\n        #text = text.strip().split() \\n        if len(text.strip().split())< 10:\\n            continue\\n        else:\\n            text1 = result[col][i]\\n            text1 = text1.strip()\\n            text = text + text1\\n    \\n    strings = strings + text\\n   # print(string)\\n    \\nEntity_recog = Named_entity_recognition(strings)\\n#print (Entity_recog)\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# concatenating all the text required into a long string to find the names of Persons in the string\n",
    "strings = ''\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident' or col=='0':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        # print(result[col][i])\n",
    "        text = result[col][i]\n",
    "        #text = text.strip().split() \n",
    "        if len(text.strip().split())< 10:\n",
    "            continue\n",
    "        else:\n",
    "            text1 = result[col][i]\n",
    "            text1 = text1.strip()\n",
    "            text = text + text1\n",
    "    \n",
    "    strings = strings + text\n",
    "   # print(string)\n",
    "    \n",
    "Entity_recog = Named_entity_recognition(strings)\n",
    "#print (Entity_recog)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# testing this code\n",
    "\n",
    "strings = ''\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident' or col=='0':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        # print(result[col][i])\n",
    "        text = result[col][i]\n",
    "        #text = text.strip().split() \n",
    "        if len(text.strip().split())< 10:\n",
    "            continue\n",
    "        else:\n",
    "            text1 = result[col][i]\n",
    "            tagged_sentence = nltk.tag.pos_tag(text1.split())\n",
    "            edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "            result[col][i] = (' '.join(edited_sentence))            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text Blob  - please skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# concatenating all the text required into a long string to find the names of Persons in the string\\nstrings = []\\n\\nfor col in result:\\n    if col=='Email_Text' or col== 'Incident' or col=='0':\\n        continue\\n    for i in range(result.shape[0]):\\n        \\n        text = result[col][i]\\n        Txtblob = TextBlob(text)\\n    \\n        for noun_phrase in Txtblob.noun_phrases:\\n            strings.append(noun_phrase)\\n        \\nstrings= list(set(strings))\\nprint (strings)\\n\\nfor col in result:\\n    if col=='Email_Text' or col== 'Incident' or col=='0':\\n        continue\\n    for i in range(result.shape[0]):\\n        \\n        text = result[col][i]\\n        Txtblob = TextBlob(text)\\n        \\n        edited_sentence = ' '.join([word for word in text.split(' ') if word not in strings ])\\n        result[col][i] = edited_sentence\\n        \\n\\nfor col in result:\\n    if col=='Email_Text' or col== 'Incident' or col=='0':\\n        continue\\n    for i in range(result.shape[0]):\\n        text = result[col][i]        \\n        edited_sentence = ' '.join([word for word in text.split(' ') if word not in strings ])\\n        result[col][i] = edited_sentence\\n\\n    #for word , pos in enumerate(Txtblob.tags):\\n    #    print (word,pos)\\n    #    if tag != '*,NNP':\\n    #        print(word)\\n    #        print(''.join(word))\\n    \\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# concatenating all the text required into a long string to find the names of Persons in the string\n",
    "strings = []\n",
    "\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident' or col=='0':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        \n",
    "        text = result[col][i]\n",
    "        Txtblob = TextBlob(text)\n",
    "    \n",
    "        for noun_phrase in Txtblob.noun_phrases:\n",
    "            strings.append(noun_phrase)\n",
    "        \n",
    "strings= list(set(strings))\n",
    "print (strings)\n",
    "\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident' or col=='0':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        \n",
    "        text = result[col][i]\n",
    "        Txtblob = TextBlob(text)\n",
    "        \n",
    "        edited_sentence = ' '.join([word for word in text.split(' ') if word not in strings ])\n",
    "        result[col][i] = edited_sentence\n",
    "        \n",
    "\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident' or col=='0':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        text = result[col][i]        \n",
    "        edited_sentence = ' '.join([word for word in text.split(' ') if word not in strings ])\n",
    "        result[col][i] = edited_sentence\n",
    "\n",
    "    #for word , pos in enumerate(Txtblob.tags):\n",
    "    #    print (word,pos)\n",
    "    #    if tag != '*,NNP':\n",
    "    #        print(word)\n",
    "    #        print(''.join(word))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileCreateError",
     "evalue": "[Errno 13] Permission denied: 'outputfile.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    624\u001b[0m             xlsx_file = ZipFile(self.filename, \"w\", compression=ZIP_DEFLATED,\n\u001b[1;32m--> 625\u001b[1;33m                                 allowZip64=self.allow_zip64)\n\u001b[0m\u001b[0;32m    626\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1206\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1208\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'outputfile.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileCreateError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-cefc2025ffe2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputfile.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2254\u001b[0m             \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2255\u001b[0m             \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2256\u001b[1;33m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2257\u001b[0m         )\n\u001b[0;32m   2258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    740\u001b[0m         )\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \"\"\"\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     def write_cells(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mFileCreateError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLargeZipFile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 raise FileSizeError(\"Filesize would require ZIP64 extensions. \"\n",
      "\u001b[1;31mFileCreateError\u001b[0m: [Errno 13] Permission denied: 'outputfile.xlsx'"
     ]
    }
   ],
   "source": [
    "result.to_excel('outputfile.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\II00083764\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"  \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    \n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    # text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\r', \"\", text)\n",
    "    text = re.sub(r\"(@.+$|\\\\..{1,3}$)\",'',text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~.!?,%'']\", \"\", text)\n",
    "    text = re.sub(r'http:\\/\\/.*', '', text)\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)    \n",
    "    text= word_tokenize(text)\n",
    "    # stemming and removing stop words\n",
    "    text = [snowball.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    # Lemmatization\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    text = ' '.join(text)\n",
    "\"\"\"\n",
    "def stemming_lemmatization(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'http:\\/\\/.*', '', text)\n",
    "    text = re.sub(r\"\\(?(\\d{3})\\)?[ .-]?(\\d{3})[ .-]?(\\d{4})\", '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "result['edited_email']=''\n",
    "\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        text = result[col][i]\n",
    "        result['edited_email'][i] = stemming_lemmatization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of words in a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"Outputfile2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of words\n",
    "text =''\n",
    "\n",
    "for col in result:\n",
    "    if col=='Email_Text' or col== 'Incident':\n",
    "        continue\n",
    "    for i in range(result.shape[0]):\n",
    "        text = text + result[col][i]        \n",
    "\n",
    "words = text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find unique words\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "txt1 = result['lastemail']\n",
    "\n",
    "txt_fitted = tf.fit_transform(txt1)\n",
    "txt_fitted = txt_fitted.toarray()\n",
    "\n",
    "txt_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
